# Tiny FlashAttention

WIP

A tiny [flash attention](https://github.com/Dao-AILab/flash-attention) implement in python, rust, cuda and c for learning purpose.

- [python version](#flash-attention-2)
    * [x] [naive pure python code](./flash_attention_py/tiny_flash_attn.py)
- [triton version](#triton-flash-attention-2)
    * [x] [triton code](./flash_attention_py/tiny_flash_attn_triton.py)
- [c version]
    * TODO: [naive pure c code]()
    * [x] [naive cuda code standalone](./flash_attention_cuda/standalone_src)
    * [x] [naive cuda code python binding](./flash_attention_cutlass/csrc/flash_attention.cu)
    * [x] [cutlass cuda code](./flash_attention_cutlass/csrc/flash_attention.cu)
- [rust version]



